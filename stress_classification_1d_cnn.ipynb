{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cipWXM7CHx5X"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Reshape, GlobalAveragePooling1D, GlobalAveragePooling2D\n",
        "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
        "#from keras.utils import np_utils\n",
        "import pickle\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36bAc1naoJXm"
      },
      "outputs": [],
      "source": [
        "!pip install np_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzoNEW3UC_34"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_t1-40RKK5op"
      },
      "outputs": [],
      "source": [
        "def feature_normalize(dataset):\n",
        "\n",
        "    mu = np.mean(dataset, axis=0)\n",
        "    sigma = np.std(dataset, axis=0)\n",
        "    return (dataset - mu)/sigma\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6VvUyCFK-Pv"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def show_confusion_matrix(validations, predictions):\n",
        "\n",
        "    matrix = metrics.confusion_matrix(validations, predictions)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(matrix,\n",
        "                cmap=\"Spectral\",\n",
        "                linecolor='white',\n",
        "                linewidths=1,\n",
        "                xticklabels=[\"Rest\", \"Stress\"],\n",
        "                yticklabels=[\"Rest\", \"Stress\"],\n",
        "                annot=True,\n",
        "                fmt=\"d\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isvdXDYqLBMP"
      },
      "outputs": [],
      "source": [
        "\n",
        "def show_basic_dataframe_info(dataframe,\n",
        "                              preview_rows=20):\n",
        "\n",
        "    \"\"\"\n",
        "    This function shows basic information for the given dataframe\n",
        "    Args:\n",
        "        dataframe: A Pandas DataFrame expected to contain data\n",
        "        preview_rows: An integer value of how many rows to preview\n",
        "    Returns:\n",
        "        Nothing\n",
        "    \"\"\"\n",
        "\n",
        "    # Shape and how many rows and columns\n",
        "    print(\"Number of columns in the dataframe: %i\" % (dataframe.shape[1]))\n",
        "    print(\"Number of rows in the dataframe: %i\\n\" % (dataframe.shape[0]))\n",
        "    print(\"First 20 rows of the dataframe:\\n\")\n",
        "    # Show first 20 rows\n",
        "    print(dataframe.head(preview_rows))\n",
        "    print(\"\\nDescription of dataframe:\\n\")\n",
        "    # Describe dataset like mean, min, max, etc.\n",
        "    # print(dataframe.describe())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6o6BTpzLE7p"
      },
      "outputs": [],
      "source": [
        "\n",
        "def read_data(file_path):\n",
        "\n",
        "    \"\"\"\n",
        "    This function reads the accelerometer data from a file\n",
        "    Args:\n",
        "        file_path: URL pointing to the CSV file\n",
        "    Returns:\n",
        "        A pandas dataframe\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    column_names = [\n",
        "                    'user-id',\n",
        "                    'timestamp',\n",
        "                    'activity',\n",
        "                    'HR',\n",
        "                    'HRV',\n",
        "                    'Raw',\n",
        "                    'EDA',\n",
        "                   ]\n",
        "    df = pd.read_csv(file_path,\n",
        "                     header=None,\n",
        "                     names=column_names)\n",
        "    # Last column has a \";\" character which must be removed ...\n",
        "   # df['z-axis'].replace(regex=True,\n",
        "   #   inplace=True,\n",
        "   #   to_replace=r';',\n",
        "   #   value=r'')\n",
        "    # ... and then this column must be transformed to float explicitly\n",
        "    #df['z-axis'] = df['z-axis'].apply(convert_to_float)\n",
        "    # This is very important otherwise the model will not fit and loss\n",
        "    # will show up as NAN\n",
        "    df.dropna(axis=0, how='any', inplace=True)\n",
        "\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4SlgBMyLIyN"
      },
      "outputs": [],
      "source": [
        "\n",
        "def convert_to_float(x):\n",
        "\n",
        "    try:\n",
        "        return np.float(x)\n",
        "    except:\n",
        "        return np.nan\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lc5xN6hTLMA4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Not used right now\n",
        "def feature_normalize(dataset):\n",
        "\n",
        "    mu = np.mean(dataset, axis=0)\n",
        "    sigma = np.std(dataset, axis=0)\n",
        "    return (dataset - mu)/sigma\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXHoRBO6LOfi"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_axis(ax, x, y, title):\n",
        "\n",
        "    ax.plot(x, y)\n",
        "    ax.set_title(title)\n",
        "    ax.xaxis.set_visible(False)\n",
        "    ax.set_ylim([min(y) - np.std(y), max(y) + np.std(y)])\n",
        "    ax.set_xlim([min(x), max(x)])\n",
        "    ax.grid(True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdwFxb73LRFI"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_activity(activity, data):\n",
        "\n",
        "    fig, (ax0, ax1, ax2, ax3) = plt.subplots(nrows=4,\n",
        "         figsize=(15, 10),\n",
        "         sharex=True)\n",
        "    plot_axis(ax0, data['timestamp'], data['HR'], 'HR')\n",
        "    plot_axis(ax1, data['timestamp'], data['HRV'], 'HRV')\n",
        "    plot_axis(ax2, data['timestamp'], data['Raw'], 'Raw')\n",
        "    plot_axis(ax3, data['timestamp'], data['EDA'], 'EDA')\n",
        "    plt.subplots_adjust(hspace=0.2)\n",
        "    fig.suptitle(activity)\n",
        "    plt.subplots_adjust(top=0.90)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoNN58cjLTdY"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def create_segments_and_labels(df, time_steps, step, label_name):\n",
        "\n",
        "    \"\"\"\n",
        "    This function receives a dataframe and returns the reshaped segments\n",
        "    of x,y,z acceleration as well as the corresponding labels\n",
        "    Args:\n",
        "        df: Dataframe in the expected format\n",
        "        time_steps: Integer value of the length of a segment that is created\n",
        "    Returns:\n",
        "        reshaped_segments\n",
        "        labels:\n",
        "    \"\"\"\n",
        "\n",
        "    # x, y, z acceleration as features\n",
        "    N_FEATURES = 4\n",
        "    # Number of steps to advance in each iteration (for me, it should always\n",
        "    # be equal to the time_steps in order to have no overlap between segments)\n",
        "    # step = time_steps\n",
        "    segments = []\n",
        "    labels = []\n",
        "    for i in range(0, len(df) - time_steps, step):\n",
        "        xs = df['HR'].values[i: i + time_steps]\n",
        "        ys = df['HRV'].values[i: i + time_steps]\n",
        "        zs = df['Raw'].values[i: i + time_steps]\n",
        "        zs1 = df['EDA'].values[i: i + time_steps]\n",
        "        label = stats.mode(df[label_name][i: i + time_steps])[0]\n",
        "        segments.append([xs, ys, zs, zs1])\n",
        "        labels.append(label)\n",
        "\n",
        "    # Bring the segments into a better shape\n",
        "    reshaped_segments = np.asarray(segments, dtype= np.float32).reshape(-1, time_steps, N_FEATURES)\n",
        "    labels = np.asarray(labels)\n",
        "\n",
        "    return reshaped_segments, labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHBHoSqzsDxB"
      },
      "outputs": [],
      "source": [
        "from keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYAHiZTzsHSl"
      },
      "outputs": [],
      "source": [
        "!pip install np_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gq0Z9LcqLYq7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ------- THE PROGRAM TO LOAD DATA AND TRAIN THE MODEL -------\n",
        "\n",
        "# Set some standard parameters upfront\n",
        "pd.options.display.float_format = '{:.1f}'.format\n",
        "sns.set() # Default seaborn look and feel\n",
        "plt.style.use('ggplot')\n",
        "print('keras version ', keras.__version__)\n",
        "\n",
        "LABELS = [\"rest\",\n",
        "          \"medium stress\",\n",
        "          \"high stress\",\n",
        "          \"low stress\",\n",
        "          \"pre_anxiety\",\n",
        "          \"anxiety_attack\",]\n",
        "# The number of steps within one time segment\n",
        "TIME_PERIODS = 300 #was 80\n",
        "# The steps to take from one segment to the next; if this value is equal to\n",
        "#TIME_PERIODS, then there is no overlap between the segments\n",
        "STEP_DISTANCE = 300 #was 40\n",
        "\n",
        "# %%\n",
        "\n",
        "print(\"\\n--- Load, inspect and transform data ---\\n\")\n",
        "\n",
        "# Load data set containing all the data from csv\n",
        "#df = read_data('Data/WISDM_ar_v1.1_raw.txt')\n",
        "df = read_data('final_dataset.csv')\n",
        "\n",
        "df1 = read_data('final_dataset.csv')\n",
        "\n",
        "\n",
        "# Describe the data\n",
        "show_basic_dataframe_info(df, 20)\n",
        "\n",
        "#df['activity'].value_counts().plot(kind='bar',\n",
        "#                                   title='Training Examples by Activity Type')\n",
        "#plt.show()\n",
        "\n",
        "#df['user-id'].value_counts().plot(kind='bar',\n",
        "#                                  title='Training Examples by User')\n",
        "#plt.show()\n",
        "\n",
        "#for activity in np.unique(df[\"activity\"]):\n",
        "#    subset = df[df[\"activity\"] == activity][:400]\n",
        "#    plot_activity(activity, subset)\n",
        "\n",
        "# Define column name of the label vector\n",
        "LABEL = \"activity\"\n",
        "# Transform the labels from String to Integer via LabelEncoder\n",
        "le = preprocessing.LabelEncoder()\n",
        "# Add a new column to the existing DataFrame with the encoded values\n",
        "df[LABEL] = le.fit_transform(df[\"activity\"].values.ravel())\n",
        "df1[LABEL] = le.fit_transform(df1[\"activity\"].values.ravel())\n",
        "\n",
        "# %%\n",
        "\n",
        "print(\"\\n--- Reshape the data into segments ---\\n\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n--- Fit the model ---\\n\")\n",
        "\n",
        "# The EarlyStopping callback monitors training accuracy:\n",
        "# if it fails to improve for two consecutive epochs,\n",
        "# training stops early\n",
        "callbacks_list = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath='exp.{epoch:02d}-{val_loss:.2f}.h5',\n",
        "        monitor='val_loss', save_best_only=True),\n",
        "    keras.callbacks.EarlyStopping(monitor='acc', patience=1)\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# Hyper-parameters\n",
        "BATCH_SIZE = 400\n",
        "EPOCHS = 10\n",
        "\n",
        "\n",
        "#for i in range(5):\n",
        "\n",
        "# Differentiate between test set and training set\n",
        "#df_test = df[df['user-id'] <= i*4]\n",
        "df_train = df\n",
        "df_test = df1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Normalize features for training data set\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#df_train['EDA'] = feature_normalize(df['EDA'])\n",
        "\n",
        "\n",
        "#df_test['HR'] = feature_normalize(df1['HR'])\n",
        "#df_test['HRV'] = feature_normalize(df1['HRV'])\n",
        "#df_test['Raw'] = feature_normalize(df1['Raw'])\n",
        "#df_test['EDA'] = feature_normalize(df1['EDA'])\n",
        "#df_test['aX'] = feature_normalize(df1['aX'])\n",
        "#df_test['aY'] = feature_normalize(df1['aY'])\n",
        "#df_test['aZ'] = feature_normalize(df1['aZ'])\n",
        "#df_test['aDIR'] = feature_normalize(df1['aDIR'])\n",
        "#df_test['gX'] = feature_normalize(df1['gX'])\n",
        "#df_test['gY'] = feature_normalize(df1['gY'])\n",
        "#df_train['gZ'] = feature_normalize(df1['gZ'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Round in order to comply to NSNumber from iOS\n",
        "#df_train = df_train.round({'x-axis': 6, 'y-axis': 6, 'z-axis': 6})\n",
        "\n",
        "# Reshape the training data into segments\n",
        "# so that they can be processed by the network\n",
        "\n",
        "x_train, y_train = create_segments_and_labels(df_train,TIME_PERIODS,STEP_DISTANCE,LABEL)\n",
        "#x_test2, y_test2 = create_segments_and_labels(df_test,TIME_PERIODS,STEP_DISTANCE,LABEL)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2)\n",
        "\n",
        "\n",
        "# Accuracy on training data: 99%\n",
        "# Accuracy on test data: 91%\n",
        "\n",
        "# %%\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n--- Reshape data to be accepted by Keras ---\\n\")\n",
        "\n",
        "# Inspect x data\n",
        "print('x_train shape: ', x_train.shape)\n",
        "# Displays (20869, 40, 3)\n",
        "print(x_train.shape[0], 'training samples')\n",
        "# Displays 20869 train samples\n",
        "\n",
        "# Inspect y data\n",
        "print('y_train shape: ', y_train.shape)\n",
        "# Displays (20869,)\n",
        "\n",
        "# Set input & output dimensions\n",
        "num_time_periods, num_sensors = x_train.shape[1], x_train.shape[2]\n",
        "num_classes = le.classes_.size\n",
        "print(list(le.classes_))\n",
        "\n",
        "# Set input_shape / reshape for Keras\n",
        "# Remark: acceleration data is concatenated in one array in order to feed\n",
        "# it properly into coreml later, the preferred matrix of shape [40,3]\n",
        "# cannot be read in with the current version of coreml (see also reshape\n",
        "# layer as the first layer in the keras model)\n",
        "input_shape = (num_time_periods*num_sensors)\n",
        "x_train = x_train.reshape(x_train.shape[0], input_shape)\n",
        "x_test = x_test.reshape(x_test.shape[0], input_shape)\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "# x_train shape: (20869, 120)\n",
        "print('input_shape:', input_shape)\n",
        "# input_shape: (120)\n",
        "\n",
        "# Convert type for Keras otherwise Keras cannot process the data\n",
        "#x_train = x_train.astype(\"float32\")\n",
        "#y_train = y_train.astype(\"float32\")\n",
        "\n",
        "#x_test2 = x_test2.astype(\"float32\")\n",
        "#y_test2 = y_test2.astype(\"float32\")\n",
        "\n",
        "\n",
        "# %%\n",
        "\n",
        "# One-hot encoding of y_train labels (only execute once!)\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "print('New y_train shape: ', y_train.shape)\n",
        "# (4173, 6)\n",
        "\n",
        "# %%\n",
        "keras.backend.set_learning_phase(0)\n",
        "\n",
        "# %%\n",
        "\n",
        "print(\"\\n--- Create neural network model ---\\n\")\n",
        "\n",
        "# 1D CNN neural network\n",
        "model_m = Sequential()\n",
        "model_m.add(Reshape((TIME_PERIODS, num_sensors,1), input_shape=(input_shape,)))\n",
        "\n",
        "model_m.add(Conv2D(100, (10,1), activation='relu', input_shape=(TIME_PERIODS, num_sensors, 1))) #added 1 at the end also changed from Conv1D to Conv2D and changed kernal size from 10 to (10,1)\n",
        "model_m.add(Conv2D(100, (10,1), activation='relu'))\n",
        "\n",
        "model_m.add(BatchNormalization(axis = -1)) #commented out for android app model\n",
        "model_m.add(MaxPooling2D(2,1)) #changed from 1d to 2d and added 1 at the end. Also was 3 for all sensors now 2 for just raw HR\n",
        "\n",
        "\n",
        "\n",
        "model_m.add(Conv2D(160, (10,1), activation='relu'))\n",
        "\n",
        "model_m.add(Conv2D(160, (10,1), activation='relu'))\n",
        "model_m.add(BatchNormalization(axis = -1)) #commented out for android app model\n",
        "\n",
        "\n",
        "model_m.add(GlobalAveragePooling2D())\n",
        "model_m.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "\n",
        "#model_m.add(LSTM(ndim=3, return_sequences=True))\n",
        "\n",
        "model_m.add(BatchNormalization(axis = -1))\n",
        "\n",
        "model_m.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "print(model_m.summary())\n",
        "\n",
        "model_m.compile(loss='binary_crossentropy',\n",
        "                optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(\"\\n--- Learning curve of model training ---\\n\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Check against test data ---\\n\")\n",
        "\n",
        "# Normalize features for training data set\n",
        "#df_test['x-axis'] = feature_normalize(df_test['x-axis'])\n",
        "#df_test['y-axis'] = feature_normalize(df_test['y-axis'])\n",
        "#df_test['z-axis'] = feature_normalize(df_test['z-axis'])\n",
        "#df_test['EDA'] = feature_normalize(df_test['EDA'])\n",
        "\n",
        "#df_test = df_test.round({'x-axis': 6, 'y-axis': 6, 'z-axis': 6})\n",
        "\n",
        "#x_test, y_test = create_segments_and_labels(df_test,\n",
        "#                                            TIME_PERIODS,\n",
        "#                                            STEP_DISTANCE,\n",
        "#                                            LABEL)\n",
        "\n",
        "\n",
        "\n",
        "# Set input_shape / reshape for Keras\n",
        "#x_test = x_test.reshape(x_test.shape[0], input_shape)\n",
        "\n",
        "#x_test = x_test.astype(\"float32\")\n",
        "#y_test = y_test.astype(\"float32\")\n",
        "\n",
        "#y_test = np_utils.to_categorical(y_test, num_classes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IHtVU0pqJNY"
      },
      "outputs": [],
      "source": [
        "df[LABEL]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnsU64NpBOux"
      },
      "outputs": [],
      "source": [
        "num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQMWXWqkBOux"
      },
      "outputs": [],
      "source": [
        "#from sklearn.model_selection import train_test_split\n",
        "#x_train, x_test3, y_train, y_test3 = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxUe-QSeBOux"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70NfGFDvBOux"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=10,  shuffle=True) # Define the split - into 2 folds\n",
        "kf.get_n_splits(x_train) # returns the number of splitting iterations in the cross-validator\n",
        "print(kf)\n",
        "i=0\n",
        "\n",
        "for train_index, test_index in kf.split(x_train):\n",
        "    print(\"Train Index: \", train_index, \"\\n\")\n",
        "    print(\"Test Index: \", test_index)\n",
        "\n",
        "    X_train, X_test, Y_train, Y_test = x_train[train_index], x_train[test_index], y_train[train_index], y_train[test_index]\n",
        "\n",
        "\n",
        "    history = model_m.fit(X_train,\n",
        "                      Y_train,\n",
        "                      batch_size=BATCH_SIZE,\n",
        "                      epochs=EPOCHS,\n",
        "                     # callbacks=callbacks_list,\n",
        "                      validation_split=0.2,\n",
        "                      verbose=1)\n",
        "\n",
        "\n",
        "    score = model_m.evaluate(X_test, Y_test, verbose=1)\n",
        "\n",
        "    print(\"\\nAccuracy on test data: %0.2f\" % score[1])\n",
        "    print(\"\\nLoss on test data: %0.2f\" % score[0])\n",
        "\n",
        "    # %%\n",
        "\n",
        "    print(\"\\n--- Confusion matrix for test data ---\\n\")\n",
        "\n",
        "    y_pred_test = model_m.predict(X_test)\n",
        "\n",
        "\n",
        "    if i==0:\n",
        "        y_pred_test_all = y_pred_test\n",
        "        Y_test_all = Y_test\n",
        "        X_test_all = X_test\n",
        "    if i>0:\n",
        "        y_pred_test_all = np.concatenate((y_pred_test_all, y_pred_test))\n",
        "        Y_test_all = np.concatenate((Y_test_all, Y_test))\n",
        "        X_test_all = np.concatenate((X_test_all, X_test))\n",
        "\n",
        "\n",
        "    # Take the class with the highest probability from the test predictions\n",
        "    max_y_pred_test = np.argmax(y_pred_test, axis=1)\n",
        "    max_y_test = np.argmax(Y_test, axis=1)\n",
        "\n",
        "    show_confusion_matrix(max_y_test, max_y_pred_test)\n",
        "\n",
        "    # %%\n",
        "\n",
        "    print(\"\\n--- Classification report for test data ---\\n\")\n",
        "\n",
        "    print(classification_report(max_y_test, max_y_pred_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9iDgsjKBOuy"
      },
      "outputs": [],
      "source": [
        "score = model_m.evaluate(x_test, y_test, verbose=1)\n",
        "\n",
        "y_pred_test = model_m.predict(x_test)\n",
        "\n",
        "max_y_pred_test_all = np.argmax(y_pred_test, axis=1)\n",
        "max_y_test_all = np.argmax(y_test, axis=1)\n",
        "\n",
        "show_confusion_matrix(max_y_test_all, max_y_pred_test_all)\n",
        "print(\"\\n--- Classification report for test data ---\\n\")\n",
        "\n",
        "print(classification_report(max_y_test, max_y_pred_test, digits=4))\n",
        "print(\"\\nAccuracy on test data: %0.5f\" % score[1])\n",
        "print(\"\\nLoss on test data: %0.5f\" % score[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGx0-pUqBOu1"
      },
      "outputs": [],
      "source": [
        "score = model_m.evaluate(X_test_all, Y_test_all, verbose=1)\n",
        "\n",
        "max_y_pred_test_all = np.argmax(y_pred_test_all, axis=1)\n",
        "max_y_test_all = np.argmax(Y_test_all, axis=1)\n",
        "\n",
        "show_confusion_matrix(max_y_test_all, max_y_pred_test_all)\n",
        "print(\"\\n--- Classification report for test data ---\\n\")\n",
        "\n",
        "print(classification_report(max_y_test, max_y_pred_test))\n",
        "print(\"\\nAccuracy on test data: %0.2f\" % score[1])\n",
        "print(\"\\nLoss on test data: %0.2f\" % score[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cikSTJe_BOu1"
      },
      "outputs": [],
      "source": [
        "print(model_m.outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ny2AeYKZBOu1"
      },
      "outputs": [],
      "source": [
        "print(model_m.inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtNnpPIukqxZ"
      },
      "outputs": [],
      "source": [
        "model_m.save('my_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5pWFs4srlL7"
      },
      "outputs": [],
      "source": [
        "x_test = tf.convert_to_tensor(X_train,dtype=tf.int8)\n",
        "\n",
        "\n",
        "#def representative_dataset():\n",
        "#    yield([tf.reshape(x_test[i],(1,1))])\n",
        "#  for i in range(300):\n",
        "\n",
        "\n",
        "def representative_dataset():\n",
        "  for data in tf.data.Dataset.from_tensor_slices((x_test)).batch(1).take(100):\n",
        "    yield [tf.dtypes.cast(data, tf.int8)]\n",
        "\n",
        "\n",
        "model = tf.keras.models.load_model('my_model.h5')\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "#converter.inference_input_type = tf.int8\n",
        "#converter.inference_output_type = tf.int8\n",
        "#converter.representative_dataset = representative_dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "open(\"stress_int8.tflite\", \"wb\").write(tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Kr3KAsylUTm"
      },
      "outputs": [],
      "source": [
        "model_m.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNlZH8n3bN5g"
      },
      "outputs": [],
      "source": [
        "!echo \"const unsigned char model[] = {\" > /content/model.h\n",
        "!cat stress_int8.tflite | xxd -i      >> /content/model.h\n",
        "!echo \"};\"                              >> /content/model.h\n",
        "\n",
        "import os\n",
        "model_h_size = os.path.getsize(\"model.h\")\n",
        "print(f\"Header file, model.h, is {model_h_size:,} bytes.\")\n",
        "print(\"\\nOpen the side panel (refresh if needed). Double click model.h to download the file.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czgOV8DilaEE"
      },
      "outputs": [],
      "source": [
        "#x_test = tf.convert_to_tensor(X_train,dtype=tf.float32)\n",
        "\n",
        "\n",
        "#def representative_dataset():\n",
        "#    yield([tf.reshape(x_test[i],(1,1))])\n",
        "#  for i in range(300):\n",
        "\n",
        "\n",
        "def representative_dataset():\n",
        "  for data in tf.data.Dataset.from_tensor_slices((x_test)).batch(1).take(100):\n",
        "    yield [tf.dtypes.cast(data, tf.float32)]\n",
        "\n",
        "\n",
        "model = tf.keras.models.load_model('my_model.h5')\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "#converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.representative_dataset = representative_dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "open(\"stress3.tflite\", \"wb\").write(tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OikAF4fOevSf"
      },
      "outputs": [],
      "source": [
        "!echo \"const unsigned char model[] = {\" > /content/model.h\n",
        "!cat stress3.tflite | xxd -i      >> /content/model.h\n",
        "!echo \"};\"                              >> /content/model.h\n",
        "\n",
        "import os\n",
        "model_h_size = os.path.getsize(\"model.h\")\n",
        "print(f\"Header file, model.h, is {model_h_size:,} bytes.\")\n",
        "print(\"\\nOpen the side panel (refresh if needed). Double click model.h to download the file.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7OcQC1ONbM0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jT_C207D317g"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as K\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "from tensorflow.keras import backend as K, models\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "\n",
        "# load model from saved chackpoint\n",
        "model = tf.keras.models.load_model('my_model.h5')\n",
        "\n",
        "# print model layers and input/outputs\n",
        "print(model.layers)\n",
        "for input in model.inputs:\n",
        "  print(input)\n",
        "for node in model.outputs:\n",
        "  print(node)\n",
        "\n",
        "# Load and transform image\n",
        "\n",
        "\n",
        "# view output\n",
        "model.predict([1,1])\n",
        "# array([[0.6071461]], dtype=float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qf-ztitz4nRJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Run the model with TensorFlow to get expected results.\n",
        "TEST_CASES = 20000\n",
        "\n",
        "# Run the model with TensorFlow Lite\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "count=0\n",
        "same = 0\n",
        "different = 0\n",
        "\n",
        "for i in range(TEST_CASES):\n",
        "  expected = model_m.predict(x_test[i:i+1])\n",
        "  interpreter.set_tensor(input_details[0][\"index\"], x_test[i:i+1, :])\n",
        "  interpreter.invoke()\n",
        "  result = interpreter.get_tensor(output_details[0][\"index\"])\n",
        "\n",
        "  # Assert if the result of TFLite model is consistent with the TF model.\n",
        "  np.testing.assert_almost_equal(expected, result,decimal=0)\n",
        "  if np.array_equal(np.round(expected, decimals=0), np.round(result, decimals=0)):\n",
        "        print(\"same\")\n",
        "        same += 1\n",
        "  else:\n",
        "        print(\"different\")\n",
        "        different += 1\n",
        "\n",
        "  count=count+1\n",
        "  print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wId7-86bIgkt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}