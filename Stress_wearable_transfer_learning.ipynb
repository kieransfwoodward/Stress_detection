{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTA5dPsCTvub"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Reshape, GlobalAveragePooling1D\n",
        "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
        "#from keras.utils import np_utils\n",
        "import pickle\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input,Flatten\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "from keras.layers.core import Dropout, Activation\n",
        "from keras.layers.pooling import GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "in_img = Input(shape=(3, 32, 32))\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from scipy import stats\n",
        "from keras.engine import InputLayer\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rsg5BYM7Tvuh"
      },
      "outputs": [],
      "source": [
        "\n",
        "def read_data(file_path):\n",
        "\n",
        "    \"\"\"\n",
        "    This function reads the accelerometer data from a file\n",
        "    Args:\n",
        "        file_path: URL pointing to the CSV file\n",
        "    Returns:\n",
        "        A pandas dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    column_names = ['user-id',\n",
        "                    'timestamp',\n",
        "                    'activity',\n",
        "                    'HR',\n",
        "                    'HRV',\n",
        "                    'raw',\n",
        "                    'EDA']\n",
        "\n",
        "\n",
        "    df = pd.read_csv(file_path,\n",
        "                     header=None,\n",
        "                     names=column_names)\n",
        "\n",
        "\n",
        "    df.dropna(axis=0, how='any', inplace=True)\n",
        "\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W39kEgtSTvui"
      },
      "outputs": [],
      "source": [
        "def gen_model():\n",
        "    in_img = Input(shape=(3, 32, 32))\n",
        "    x = Convolution2D(12, 3, 3, subsample=(2, 2), border_mode='valid', name='conv1')(in_img)\n",
        "    x = Activation('relu', name='relu_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool1')(x)\n",
        "    x = Convolution2D(3, 1, 1, border_mode='valid', name='conv2')(x)\n",
        "    x = Activation('relu', name='relu_conv2')(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    o = Activation('softmax', name='loss')(x)\n",
        "    model = Model(input=in_img, output=[o])\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LM6U2ToHTvuj"
      },
      "outputs": [],
      "source": [
        "\n",
        "def show_basic_dataframe_info(dataframe,\n",
        "                              preview_rows=20):\n",
        "\n",
        "    \"\"\"\n",
        "    This function shows basic information for the given dataframe\n",
        "    Args:\n",
        "        dataframe: A Pandas DataFrame expected to contain data\n",
        "        preview_rows: An integer value of how many rows to preview\n",
        "    Returns:\n",
        "        Nothing\n",
        "    \"\"\"\n",
        "\n",
        "    # Shape and how many rows and columns\n",
        "    print(\"Number of columns in the dataframe: %i\" % (dataframe.shape[1]))\n",
        "    print(\"Number of rows in the dataframe: %i\\n\" % (dataframe.shape[0]))\n",
        "    print(\"First 20 rows of the dataframe:\\n\")\n",
        "    # Show first 20 rows\n",
        "    print(dataframe.head(preview_rows))\n",
        "    print(\"\\nDescription of dataframe:\\n\")\n",
        "    # Describe dataset like mean, min, max, etc.\n",
        "    # print(dataframe.describe())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q41DfmhkTvuj"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_activity(activity, data):\n",
        "\n",
        "    fig, (ax0, ax1, ax2) = plt.subplots(nrows=3,\n",
        "         figsize=(15, 10),\n",
        "         sharex=True)\n",
        "    plot_axis(ax0, data['timestamp'], data['HR'], 'HR')\n",
        "    plot_axis(ax1, data['timestamp'], data['HRV'], 'HRV')\n",
        "    plot_axis(ax2, data['timestamp'], data['EDA'], 'EDA')\n",
        "    plt.subplots_adjust(hspace=0.2)\n",
        "    fig.suptitle(activity)\n",
        "    plt.subplots_adjust(top=0.90)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3Lhg9PkTvuk"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_axis(ax, x, y, title):\n",
        "\n",
        "    ax.plot(x, y)\n",
        "    ax.set_title(title)\n",
        "    ax.xaxis.set_visible(False)\n",
        "    ax.set_ylim([min(y) - np.std(y), max(y) + np.std(y)])\n",
        "    ax.set_xlim([min(x), max(x)])\n",
        "    ax.grid(True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJMfX-TETvuk"
      },
      "outputs": [],
      "source": [
        "\n",
        "def feature_normalize(dataset):\n",
        "\n",
        "    mu = np.mean(dataset, axis=0)\n",
        "    sigma = np.std(dataset, axis=0)\n",
        "    return (dataset - mu)/sigma\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxlnvk_1Tvuk"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def create_segments_and_labels(df, time_steps, step, label_name):\n",
        "\n",
        "    \"\"\"\n",
        "    This function receives a dataframe and returns the reshaped segments\n",
        "    of x,y,z acceleration as well as the corresponding labels\n",
        "    Args:\n",
        "        df: Dataframe in the expected format\n",
        "        time_steps: Integer value of the length of a segment that is created\n",
        "    Returns:\n",
        "        reshaped_segments\n",
        "        labels:\n",
        "    \"\"\"\n",
        "\n",
        "    # x, y, z acceleration as features\n",
        "    N_FEATURES = 4\n",
        "    # Number of steps to advance in each iteration (for me, it should always\n",
        "    # be equal to the time_steps in order to have no overlap between segments)\n",
        "    # step = time_steps\n",
        "    segments = []\n",
        "    labels = []\n",
        "    for i in range(0, len(df) - time_steps, step):\n",
        "        zs7 = df['EDA'].values[i: i + time_steps]\n",
        "        zs8 = df['HR'].values[i: i + time_steps]\n",
        "        zs9 = df['HRV'].values[i: i + time_steps]\n",
        "        zs10 = df['raw'].values[i: i + time_steps]\n",
        "\n",
        "        # Retrieve the most often used label in this segment\n",
        "        label = stats.mode(df[label_name][i: i + time_steps])[0][0]\n",
        "        segments.append([zs7, zs8, zs9, zs10])\n",
        "        labels.append(label)\n",
        "\n",
        "    # Bring the segments into a better shape\n",
        "    reshaped_segments = np.asarray(segments, dtype= np.float32).reshape(-1, time_steps, N_FEATURES)\n",
        "    labels = np.asarray(labels)\n",
        "\n",
        "    return reshaped_segments, labels\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kqe119cHTvul"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9PW27UVTvul"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ------- THE PROGRAM TO LOAD DATA AND TRAIN THE MODEL -------\n",
        "\n",
        "# Set some standard parameters upfront\n",
        "pd.options.display.float_format = '{:.1f}'.format\n",
        "sns.set() # Default seaborn look and feel\n",
        "plt.style.use('ggplot')\n",
        "print('keras version ', keras.__version__)\n",
        "\n",
        "LABELS = [\"0\",\n",
        "          \"1\"]\n",
        "# The number of steps within one time segment\n",
        "TIME_PERIODS = 100 #was 80\n",
        "# The steps to take from one segment to the next; if this value is equal to\n",
        "#TIME_PERIODS, then there is no overlap between the segments\n",
        "STEP_DISTANCE = 20 #was 40\n",
        "\n",
        "# %%\n",
        "\n",
        "print(\"\\n--- Load, inspect and transform data ---\\n\")\n",
        "\n",
        "# Load data set containing all the data from csv\n",
        "#df = read_data('Data/WISDM_ar_v1.1_raw.txt')\n",
        "#df = read_data('combined_clean3.csv')\n",
        "df = read_data('user6.csv')\n",
        "df1 = read_data('user6.csv')\n",
        "\n",
        "\n",
        "# Describe the data\n",
        "show_basic_dataframe_info(df, 20)\n",
        "\n",
        "#df['activity'].value_counts().plot(kind='bar',\n",
        "#                                   title='Training Examples by Activity Type')\n",
        "#plt.show()\n",
        "\n",
        "#df['user-id'].value_counts().plot(kind='bar',\n",
        "#                                  title='Training Examples by User')\n",
        "#plt.show()\n",
        "\n",
        "#for activity in np.unique(df[\"activity\"]):\n",
        "#    subset = df[df[\"activity\"] == activity][:180]\n",
        "#    plot_activity(activity, subset)\n",
        "\n",
        "# Define column name of the label vector\n",
        "LABEL = \"activity\"\n",
        "# Transform the labels from String to Integer via LabelEncoder\n",
        "le = preprocessing.LabelEncoder()\n",
        "# Add a new column to the existing DataFrame with the encoded values\n",
        "df[LABEL] = le.fit_transform(df[\"activity\"].values.ravel())\n",
        "df1[LABEL] = le.fit_transform(df1[\"activity\"].values.ravel())\n",
        "\n",
        "# %%\n",
        "\n",
        "print(\"\\n--- Reshape the data into segments ---\\n\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n--- Fit the model ---\\n\")\n",
        "\n",
        "# The EarlyStopping callback monitors training accuracy:\n",
        "# if it fails to improve for two consecutive epochs,\n",
        "# training stops early\n",
        "callbacks_list = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath='best_model.{epoch:02d}-{val_loss:.2f}.h5',\n",
        "        monitor='val_loss', save_best_only=True),\n",
        "    keras.callbacks.EarlyStopping(monitor='acc', patience=1)\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# Hyper-parameters\n",
        "BATCH_SIZE = 400\n",
        "EPOCHS = 50\n",
        "\n",
        "\n",
        "#for i in range(5):\n",
        "\n",
        "# Differentiate between test set and training set\n",
        "#df_test = df[df['user-id'] <= i*4]\n",
        "df_train = df\n",
        "df_test = df\n",
        "\n",
        "df_train1 = df1\n",
        "df_test1 = df1\n",
        "\n",
        "\n",
        "# Normalize features for training data set\n",
        "#df_train['touch'] = feature_normalize(df['touch'])\n",
        "#df_train['aX'] = feature_normalize(df['aX'])\n",
        "#df_train['aY'] = feature_normalize(df['aY'])\n",
        "#df_train['aZ'] = feature_normalize(df['aZ'])\n",
        "#df_train['gX'] = feature_normalize(df['gX'])\n",
        "#df_train['gY'] = feature_normalize(df['gY'])\n",
        "#df_train['gZ'] = feature_normalize(df['gZ'])\n",
        "#df_train['aSqrt'] = feature_normalize(df['aSqrt'])\n",
        "df_train['EDA'] = feature_normalize(df['EDA'])\n",
        "df_train['HR'] = feature_normalize(df['HR'])\n",
        "df_train['HRV'] = feature_normalize(df['HRV'])\n",
        "df_train['raw'] = feature_normalize(df['raw'])\n",
        "\n",
        "\n",
        "#df_train['gX'] = feature_normalize(df['gX'])\n",
        "#df_train['gY'] = feature_normalize(df['gY'])\n",
        "#df_train['gZ'] = feature_normalize(df['gZ'])\n",
        "#df_train['aSqrt'] = feature_normalize(df['aSqrt'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#df_train1['touch'] = feature_normalize(df1['touch'])\n",
        "#df_train1['aX'] = feature_normalize(df1['aX'])\n",
        "#df_train1['aY'] = feature_normalize(df1['aY'])\n",
        "#df_train1['aZ'] = feature_normalize(df1['aZ'])\n",
        "#df_train1['gX'] = feature_normalize(df1['gX'])\n",
        "#df_train1['gY'] = feature_normalize(df1['gY'])\n",
        "#df_train1['gZ'] = feature_normalize(df1['gZ'])\n",
        "#df_train1['aSqrt'] = feature_normalize(df1['aSqrt'])\n",
        "\n",
        "df_train1['EDA'] = feature_normalize(df1['EDA'])\n",
        "df_train1['HR'] = feature_normalize(df1['HR'])\n",
        "df_train1['HRV'] = feature_normalize(df1['HRV'])\n",
        "df_train1['raw'] = feature_normalize(df1['raw'])\n",
        "\n",
        "#df_train1['gX'] = feature_normalize(df1['gX'])\n",
        "#df_train1['gY'] = feature_normalize(df1['gY'])\n",
        "#df_train1['gZ'] = feature_normalize(df1['gZ'])\n",
        "#df_train1['aSqrt'] = feature_normalize(df1['aSqrt'])\n",
        "\n",
        "\n",
        "# Round in order to comply to NSNumber from iOS\n",
        "#df_train = df_train.round({'x-axis': 6, 'y-axis': 6, 'z-axis': 6})\n",
        "\n",
        "# Reshape the training data into segments\n",
        "# so that they can be processed by the network\n",
        "\n",
        "x_train, y_train = create_segments_and_labels(df_train,TIME_PERIODS,STEP_DISTANCE,LABEL)\n",
        "\n",
        "x_train, x_test1, y_train, y_test1 = train_test_split(x_train, y_train, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "Xtest, Ytest = create_segments_and_labels(df_train1,TIME_PERIODS,STEP_DISTANCE,LABEL)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqeapQ60Tvum"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku_FABQXTvum"
      },
      "outputs": [],
      "source": [
        "model = keras.models.load_model('user6.h5')\n",
        "#model = keras.models.load_model('experiment.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTgCfByJTvun"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02BwYwD_Tvun"
      },
      "outputs": [],
      "source": [
        "#model._layers.pop(0)\n",
        "#model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hd1sa7E8Tvun"
      },
      "outputs": [],
      "source": [
        "for i in range(8): #was(8)\n",
        "    model.layers[i].trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcJAdU9MTvuo"
      },
      "outputs": [],
      "source": [
        "for i in range(4,8):\n",
        "    model.layers[i].trainable = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOxNMmyYTvuo"
      },
      "outputs": [],
      "source": [
        "for l in model.layers:\n",
        "    print(l.name, l.trainable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiyqbExCTvuo"
      },
      "outputs": [],
      "source": [
        "#ll = model.layers[6].output #was 6\n",
        "#ll = BatchNormalization(axis = -1)(ll)\n",
        "#ll = Dense(32)(ll)\n",
        "#ll = Dense(160)(ll) #was 64 now 160\n",
        "#ll = Dropout(0.5)(ll)\n",
        "#ll = Dense(2,activation=\"sigmoid\")(ll)\n",
        "\n",
        "ll = model.layers[6].output #was 6\n",
        "ll = BatchNormalization(axis = -1)(ll)\n",
        "ll = Dense(64)(ll)\n",
        "ll = Dense(32)(ll) #was 64 now 160\n",
        "ll = Dropout(0.5)(ll)\n",
        "ll = Dense(2,activation=\"sigmoid\")(ll)\n",
        "\n",
        "\n",
        "# The number of steps within one time segment\n",
        "#TIME_PERIODS = 100 #was 80\n",
        "# The steps to take from one segment to the next; if this value is equal to\n",
        "#TIME_PERIODS, then there is no overlap between the segments\n",
        "#STEP_DISTANCE = 20 #was 40\n",
        "# Set input & output dimensions\n",
        "#num_time_periods, num_sensors = x_train.shape[1], x_train.shape[2]\n",
        "#num_classes = le.classes_.size\n",
        "print(list(le.classes_))\n",
        "\n",
        "# Set input_shape / reshape for Keras\n",
        "# Remark: acceleration data is concatenated in one array in order to feed\n",
        "# it properly into coreml later, the preferred matrix of shape [40,3]\n",
        "# cannot be read in with the current version of coreml (see also reshape\n",
        "# layer as the first layer in the keras model)\n",
        "\n",
        "#input_shape = (num_time_periods*num_sensors)\n",
        "\n",
        "#inlayer = model.layers[0].input\n",
        "#inlayer = Reshape((TIME_PERIODS, num_sensors), input_shape=(input_shape,)) (inlayer)\n",
        "\n",
        "\n",
        "#input_layer = InputLayer(input_shape=(41182, 400), name=\"input_1\")\n",
        "#model.layers[0] = input_layer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m67DommlTvup"
      },
      "outputs": [],
      "source": [
        "new_model = Model(inputs=model.input,outputs=ll)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04YqXAufTvup"
      },
      "outputs": [],
      "source": [
        "new_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmuCImJJTvup"
      },
      "outputs": [],
      "source": [
        "for l in new_model.layers:\n",
        "    print(l.name, l.trainable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntcY9HvETvup"
      },
      "outputs": [],
      "source": [
        "#new_model.layers[11].get_weights()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQ4YBVy6Tvuq"
      },
      "outputs": [],
      "source": [
        "#X_train, X_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.33)\n",
        "\n",
        "\n",
        "# Accuracy on training data: 99%\n",
        "# Accuracy on test data: 91%\n",
        "\n",
        "# %%\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n--- Reshape data to be accepted by Keras ---\\n\")\n",
        "\n",
        "# Inspect x data\n",
        "print('x_train shape: ', x_train.shape)\n",
        "# Displays (20869, 40, 3)\n",
        "print(x_train.shape[0], 'training samples')\n",
        "# Displays 20869 train samples\n",
        "\n",
        "# Inspect y data\n",
        "print('y_train shape: ', y_train.shape)\n",
        "# Displays (20869,)\n",
        "\n",
        "# Set input & output dimensions\n",
        "num_time_periods, num_sensors = x_train.shape[1], x_train.shape[2]\n",
        "num_classes = le.classes_.size\n",
        "print(list(le.classes_))\n",
        "\n",
        "# Set input_shape / reshape for Keras\n",
        "# Remark: acceleration data is concatenated in one array in order to feed\n",
        "# it properly into coreml later, the preferred matrix of shape [40,3]\n",
        "# cannot be read in with the current version of coreml (see also reshape\n",
        "# layer as the first layer in the keras model)\n",
        "input_shape = (num_time_periods*num_sensors)\n",
        "x_train = x_train.reshape(x_train.shape[0], input_shape)\n",
        "x_test1 = x_test1.reshape(x_test1.shape[0], input_shape)\n",
        "Xtest = Xtest.reshape(Xtest.shape[0], input_shape)\n",
        "print('x_train shape:', x_train.shape)\n",
        "# x_train shape: (20869, 120)\n",
        "print('input_shape:', input_shape)\n",
        "# input_shape: (120)\n",
        "\n",
        "# Convert type for Keras otherwise Keras cannot process the data\n",
        "x_train = x_train.astype(\"float32\")\n",
        "Xtest = Xtest.astype(\"float32\")\n",
        "y_train = y_train.astype(\"float32\")\n",
        "Ytest = Ytest.astype(\"float32\")\n",
        "\n",
        "x_test1 = x_test1.astype(\"float32\")\n",
        "y_test1 = y_test1.astype(\"float32\")\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "Ytest =  np_utils.to_categorical(Ytest, num_classes)\n",
        "y_test1 =  np_utils.to_categorical(y_test1, num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxhG8iLlTvuq"
      },
      "outputs": [],
      "source": [
        "num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzE5HJy9Tvuq"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZZX-G-6Tvuq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def show_confusion_matrix(validations, predictions):\n",
        "\n",
        "    matrix = metrics.confusion_matrix(validations, predictions)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(matrix,\n",
        "                cmap=\"coolwarm\",\n",
        "                linecolor='white',\n",
        "                linewidths=1,\n",
        "                xticklabels=LABELS,\n",
        "                yticklabels=LABELS,\n",
        "                annot=True,\n",
        "                fmt=\"d\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9goDJQ-KTvur"
      },
      "outputs": [],
      "source": [
        "#need to do model.fit\n",
        "\n",
        "BATCH_SIZE = 400\n",
        "EPOCHS = 50\n",
        "\n",
        "new_model.compile(loss='binary_crossentropy',\n",
        "                optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=10,  shuffle=True) # Define the split - into 2 folds\n",
        "kf.get_n_splits(x_train) # returns the number of splitting iterations in the cross-validator\n",
        "print(kf)\n",
        "i=0\n",
        "\n",
        "for train_index, test_index in kf.split(x_train):\n",
        "    print(\"Train Index: \", train_index, \"\\n\")\n",
        "    print(\"Test Index: \", test_index)\n",
        "\n",
        "    X_train, X_test, Y_train, Y_test = x_train[train_index], x_train[test_index], y_train[train_index], y_train[test_index]\n",
        "#X_train, X_test, Y_train, Y_test = x_train, Xtest, y_train, Ytest\n",
        "\n",
        "    print(\"Train Index: \", Y_train)\n",
        "\n",
        "    history = new_model.fit(X_train,\n",
        "                      Y_train,\n",
        "                      batch_size=BATCH_SIZE,\n",
        "                      epochs=EPOCHS,\n",
        "                      callbacks=callbacks_list,\n",
        "                      validation_split=0.2,\n",
        "                      verbose=1)\n",
        "\n",
        "\n",
        "    score = new_model.evaluate(X_test, Y_test, verbose=1)\n",
        "\n",
        "    print(\"\\nAccuracy on test data: %0.2f\" % score[1])\n",
        "    print(\"\\nLoss on test data: %0.2f\" % score[0])\n",
        "\n",
        "    # %%\n",
        "\n",
        "    print(\"\\n--- Confusion matrix for test data ---\\n\")\n",
        "\n",
        "    y_pred_test = new_model.predict(X_test)\n",
        "\n",
        "\n",
        "\n",
        "    if i==0:\n",
        "        y_pred_test_all = y_pred_test\n",
        "        Y_test_all = Y_test\n",
        "        X_test_all = X_test\n",
        "    if i>0:\n",
        "        y_pred_test_all = np.concatenate((y_pred_test_all, y_pred_test))\n",
        "        Y_test_all = np.concatenate((Y_test_all, Y_test))\n",
        "        X_test_all = np.concatenate((X_test_all, X_test))\n",
        "\n",
        "\n",
        "    # Take the class with the highest probability from the test predictions\n",
        "    max_y_pred_test = np.argmax(y_pred_test, axis=1)\n",
        "    max_y_test = np.argmax(Y_test, axis=1)\n",
        "\n",
        "    show_confusion_matrix(max_y_test, max_y_pred_test)\n",
        "\n",
        "    # %%\n",
        "\n",
        "    print(\"\\n--- Classification report for test data ---\\n\")\n",
        "\n",
        "    print(classification_report(max_y_test, max_y_pred_test))\n",
        "    i = i +1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "HPDdSmriTvur"
      },
      "outputs": [],
      "source": [
        "score = new_model.evaluate(X_test_all, Y_test_all, verbose=1)\n",
        "\n",
        "max_y_pred_test_all = np.argmax(y_pred_test_all, axis=1)\n",
        "max_y_test_all = np.argmax(Y_test_all, axis=1)\n",
        "\n",
        "show_confusion_matrix(max_y_test_all, max_y_pred_test_all)\n",
        "print(\"\\n--- Classification report for test data ---\\n\")\n",
        "\n",
        "print(classification_report(max_y_test_all, max_y_pred_test_all))\n",
        "print(\"\\nAccuracy on test data: %0.3f\" % score[1])\n",
        "print(\"\\nLoss on test data: %0.3f\" % score[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "sbdfX9ggTvur"
      },
      "outputs": [],
      "source": [
        "score = new_model.evaluate(x_test1, y_test1, verbose=1)\n",
        "\n",
        "y_pred_test = new_model.predict(x_test1)\n",
        "max_y_pred_test = np.argmax(y_pred_test, axis=1)\n",
        "max_y_test = np.argmax(y_test1, axis=1)\n",
        "\n",
        "show_confusion_matrix(max_y_test, max_y_pred_test)\n",
        "\n",
        "    # %%\n",
        "\n",
        "print(\"\\n--- Classification report for test data ---\\n\")\n",
        "\n",
        "print(classification_report(max_y_test, max_y_pred_test))\n",
        "\n",
        "\n",
        "print(\"\\nAccuracy on test data: %0.3f\" % score[1])\n",
        "print(\"\\nLoss on test data: %0.3f\" % score[0])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}